# ğŸ“Š MercadoPago - Data Pipeline para Value Props

Este proyecto desarrolla un pipeline de ingenierÃ­a de datos en Python para preparar un dataset que serÃ¡ utilizado en un modelo de Machine Learning de **MercadoPago**. El objetivo es predecir el orden Ã³ptimo de propuestas de valor (*value props*).

---

## ğŸ§  Objetivo

Construir un dataset final que contenga, para cada impresiÃ³n (*print*) de la Ãºltima semana:

- Si hubo clic (*tap*) o no.
- CuÃ¡ntas veces el usuario vio esa *value prop* en las 3 semanas previas.
- CuÃ¡ntos clics hizo sobre esa *value prop* en las 3 semanas previas.
- CuÃ¡ntos pagos hizo relacionados con esa *value prop* en ese perÃ­odo.
- CuÃ¡nto dinero gastÃ³ en esa *value prop* en ese perÃ­odo.

---
 Decisiones TÃ©cnicas Tomadas
Durante la exploraciÃ³n y el desarrollo del pipeline de datos, se tomaron las siguientes decisiones clave para asegurar la calidad, eficiencia y escalabilidad del dataset final:

1. ExpansiÃ³n de columnas estructuradas:

Los archivos prints.json y taps.json contenÃ­an una columna event_data con informaciÃ³n estructurada (tipo Struct). Esta fue expandida en columnas planas (position, value_prop, etc.) para facilitar el anÃ¡lisis y la construcciÃ³n de features.

2. EstandarizaciÃ³n temporal:

Se cambio el nombre de la columna de fecha (pay_date) por (day) en el dataframe del archivo pays.csv para que fuera igual que en todos los dataframes y se normalizo a un formato date, se generaron columnas auxiliares como week. Esto permitiÃ³ aplicar filtros por semana y construir ventanas de tiempo mÃ³viles de manera consistente.

3. Limpieza de datos:
Se aplicÃ³ una funciÃ³n de limpieza uniforme que elimina duplicados y nulos, asegurando que los datos a procesar sean Ãºnicos y relevantes. Este paso fue especialmente importante para evitar sobrecontar impresiones, clics o pagos.

4. IdentificaciÃ³n de clics (taps):

Para determinar si una impresiÃ³n fue clickeada, se creÃ³ una clave Ãºnica combinando day, user_id y value_prop. Esta clave fue usada para realizar un left join eficiente entre prints y taps, generando la variable binaria clicked.

5. Agregaciones histÃ³ricas:

Se construyeron variables histÃ³ricas para cada combinaciÃ³n (user_id, value_prop) en las 3 semanas previas a cada impresiÃ³n de la Ãºltima semana:

Cantidad de impresiones (n_prints_prev3w)

Cantidad de clics (n_taps_prev3w)

Cantidad de pagos (n_pays_prev3w)

Total gastado (sum_pays_prev3w)

Estas features permiten capturar el comportamiento pasado del usuario y serÃ¡n claves para el modelo predictivo.

6. ModularizaciÃ³n del script y legibilidad profesional:

El script feature_builder_polars.py fue diseÃ±ado en forma modular, utilizando funciones limpias y reutilizables para cada paso del proceso. Esto facilita el mantenimiento, testing y futura extensiÃ³n del pipeline.

Las funciones clave implementadas son:

FunciÃ³n	PropÃ³sito
build_join_key(df): Crea una clave Ãºnica combinando day, user_id, value_prop para detectar clics.

get_last_week_prints(prints):   	Filtra los prints que ocurrieron en la Ãºltima semana del dataset.

mark_clicks(last_week_prints, taps):	Marca con 1 los prints que tuvieron un tap en la misma fecha.

get_historical_aggregates(...):	Calcula agregaciones histÃ³ricas (prints, taps y pagos en 3 semanas previas).

build_training_dataset(...):	Combina todos los pasos anteriores para construir el dataset final.

main():	Ejecuta el pipeline completo, desde la carga hasta la escritura en formato Parquet.

7. Eficiencia y formato del output:

Se utilizÃ³ la librerÃ­a Polars por su alto rendimiento en procesamiento de datos tabulares. El dataset final se guardÃ³ en formato Parquet, lo cual permite una compresiÃ³n eficiente y una lectura rÃ¡pida para tareas posteriores de modelado.


---
## ğŸ“‚ Estructura del proyecto
```bash
MercadoPago/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_loader_polars.py # MÃ³dulo para cargar y limpiar datos
â”‚ â”œâ”€â”€ feature_builder_polars.py # Pipeline completo de construcciÃ³n del dataset
â”œâ”€â”€ data/ # Archivos de entrada (prints.json, taps.json, pays.csv)
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ exploratory_analysis.ipynb # Cuaderno donde se realizo el analisis exploratorio de datos
â”‚ â”œâ”€â”€ test_final_dataset.ipynb # Cuaderno donde se verifico el dataset final
â”œâ”€â”€ output/
â”‚ â””â”€â”€ final_dataset.parquet # Dataset final generado
â”œâ”€â”€ requirements.txt # Dependencias
â””â”€â”€ README.md # Este archivo
```

---

## âš™ï¸ Requisitos

Python â‰¥ 3.9

Instalar las dependencias necesarias con:

```bash
pip install -r requirements.txt

```
CÃ³mo ejecutar el pipeline

Coloca los archivos prints.json, taps.json y pays.csv en la carpeta data/.


Ejecuta el script principal:
```bash
python src/feature_builder_polars.py
```
El dataset final se guardarÃ¡ automÃ¡ticamente como archivo .parquet comprimido en:
```bash
output/final_dataset.parquet
